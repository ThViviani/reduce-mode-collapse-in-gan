{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./reduce-mode-collapse-in-gan\n",
    "!git clone -b synthetic_experiment https://github.com/ThViviani/reduce-mode-collapse-in-gan.git\n",
    "\n",
    "import sys; sys.path.append('./reduce-mode-collapse-in-gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./reduce-mode-collapse-in-gan/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./gan\n",
    "!git clone https://github.com/tntrung/gan.git\n",
    "\n",
    "sys.path.append('./gan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare 2D data from dist-gan\n",
    "https://github.com/tntrung/gan/tree/master/distgan_toy2d  \n",
    "I have problems when i try import the distgan_toy2d module, so i copied some code in this cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 500\n",
    "SEED = 42\n",
    "WANDB_TOKEN = '' # input your token\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_toydata(toyfile):\n",
    "    fid = open(toyfile,'r')\n",
    "    lines = fid.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.replace('[', '')\n",
    "        line = line.replace(']', '')\n",
    "        data.append([float(curr_num) for curr_num in line.split()])\n",
    "    fid.close()\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxabs(a, axis=None):\n",
    "    amax = a.max(axis)\n",
    "    amin = a.min(axis)\n",
    "    return np.where(-amin > amax, amin, amax)\n",
    "\n",
    "def normalize_toydata(toydata, centroids, var):\n",
    "    centroids = (centroids/maxabs(np.float32(toydata))+1)/2\n",
    "    var = (var/maxabs(toydata))/np.sqrt(2)\n",
    "    toydata = (toydata/maxabs(toydata)+1)/2\n",
    "    toydata_size = len(toydata)\n",
    "    return toydata, centroids, var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_centroids = np.array([np.array([i, j]) for i, j in itertools.product(range(-4, 5, 2),\n",
    "                                                                    range(-4, 5, 2))])\n",
    "toydata = read_toydata('gan/distgan_toy2d/toy_data/toydatav2.txt')\n",
    "toydata, grid_centroids, VAR = normalize_toydata(toydata, grid_centroids, VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(toydata[:,0], toydata[:,1], color='b')\n",
    "plt.scatter(grid_centroids[:,0], grid_centroids[:,1], marker='x', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_right_corner_mode = toydata[(np.linalg.norm(toydata - grid_centroids[-1], axis=1) <= VAR)]\n",
    "\n",
    "plt.scatter(toydata[:,0], toydata[:,1], color='b')\n",
    "plt.scatter(upper_right_corner_mode[:,0], upper_right_corner_mode[:,1], color='g')\n",
    "plt.scatter(grid_centroids[:,0], grid_centroids[:,1], marker='x', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mode_covered(data, centroids, var):\n",
    "    mode_covered = [0 for _ in range(len(centroids))]\n",
    "    for i in range(len(centroids)):\n",
    "        subdata = data - centroids[i]\n",
    "        distance = np.linalg.norm(subdata,axis=1)\n",
    "        point_in_mode = (distance<=var).sum()\n",
    "        mode_covered[i] = point_in_mode\n",
    "    return np.array(mode_covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(evaluate_mode_covered(toydata, grid_centroids, VAR) >= 20).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_labels = torch.ones(toydata.shape[0])\n",
    "train_dataset = TensorDataset(torch.FloatTensor(toydata), dummy_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))[0]\n",
    "plt.scatter(b[:,0], b[:,0])\n",
    "plt.scatter(grid_centroids[:,0], grid_centroids[:,1], marker='x', color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_options import TrainOptions\n",
    "\n",
    "\n",
    "train_options = TrainOptions(\n",
    "    latent_dim=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-3,\n",
    "    betas=(0.8, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.synthetic_adversarial_trainer import *\n",
    "from models.discriminators import Critic2D\n",
    "from models.utils import MLP\n",
    "from models.generators import Generator2D\n",
    "\n",
    "\n",
    "def create_model_fn(model_class, centroids, var, opt, use_encoder=False, use_r1r2=False):\n",
    "    def wrapper():\n",
    "        if model_class.__name__.find('Rp') != -1:\n",
    "            return model_class(\n",
    "                critic=Critic2D(output_dim=1),\n",
    "                generator=Generator2D(),\n",
    "                encoder=MLP() if use_encoder else None,\n",
    "                prior_type='uniform',\n",
    "                use_r1r2_penalty=use_r1r2,\n",
    "                centroids=centroids,\n",
    "                var=var,\n",
    "                opt=opt\n",
    "            )\n",
    "        else:\n",
    "            return model_class(\n",
    "                critic=Critic2D(output_dim=1),\n",
    "                generator=Generator2D(),\n",
    "                encoder=MLP() if use_encoder else None,\n",
    "                prior_type='uniform',\n",
    "                centroids=centroids,\n",
    "                var=var,\n",
    "                opt=opt\n",
    "            )\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'StandardGAN': create_model_fn(\n",
    "        SyntheticVanilaGAN, centroids=grid_centroids, var=VAR, opt=train_options\n",
    "    ),\n",
    "    'NEVanilaGAN': create_model_fn(\n",
    "        SynthNEVanilaGAN, use_encoder=True, centroids=grid_centroids, var=VAR, opt=train_options\n",
    "    ),\n",
    "    'DistVanilaGAN': create_model_fn(SynthDistVanilaGAN, use_encoder=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'DpVanilaGAN': create_model_fn(SynthDpVanilaGAN, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'RpGAN': create_model_fn(SyntheticRpGAN, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'NERpGAN': create_model_fn(SynthNERpGAN, use_encoder=True, centroids=grid_centroids, var=VAR, opt=train_options),    \n",
    "    'DistRpGAN': create_model_fn(SynthDistRpGAN, use_encoder=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'DpRpGAN': create_model_fn(SynthDpRpGAN, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'RpGAN_R1R2': create_model_fn(SyntheticRpGAN, use_r1r2=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'DistRpGAN+R1R2': create_model_fn(SynthDistRpGAN, use_encoder=True, use_r1r2=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'NERpGAN+R1R2': create_model_fn(SynthNERpGAN, use_encoder=True, use_r1r2=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'DpRpGAN+R1R2': create_model_fn(SynthDpRpGAN, use_r1r2=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'NEVanilaGAN_hat': create_model_fn(SynthNEhatVanilaGAN, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'NERpGAN_hat': create_model_fn(SynthNEhatRpGAN, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "    'NERpGAN_hat+R1R2': create_model_fn(SynthNEhatRpGAN, use_r1r2=True, centroids=grid_centroids, var=VAR, opt=train_options),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=WANDB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\n",
    "    'registered modes', \n",
    "    'registered samples', \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model_fn in EXPERIMENTS.items():\n",
    "    print(f\"Running {name}\")\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project='Synthetic2D_vkr',\n",
    "        save_dir='',\n",
    "        log_model=True,\n",
    "        name=name + \"_\" + str(datetime.now())\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        logger=wandb_logger,\n",
    "        deterministic=True,\n",
    "        callbacks=[LearningRateMonitor(logging_interval='epoch')]\n",
    "    )\n",
    "\n",
    "    model = model_fn()\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "    results.loc[name] = model.compute_modes_covered()\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
